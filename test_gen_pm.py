import torch
import os
import argparse
from ml_collections import ConfigDict
from transformers import AutoTokenizer

# –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def create_test_config(device='cpu'):
    config = ConfigDict()
    
    # Device
    config.device = device
    
    # Model config
    config.model = ConfigDict()
    config.model.encoder_link = "bert-base-cased"
    config.model.ema_rate = 0.9999
    
    # Decoder config
    config.decoder = ConfigDict()
    config.decoder.mode = "transformer"
    config.decoder.num_hidden_layers = 6
    config.decoder.is_conditional = True
    config.decoder.decoder_path = "datasets/rocstories/decoder-bert-base-cased-80-transformer.pth"
    
    # Score estimator config
    config.se_config = ConfigDict()
    config.se_config.hidden_size = 768
    config.se_config.num_hidden_layers = 12
    config.se_config.vocab_size = 28996
    config.se_config.max_position_embeddings = 512
    config.se_config.num_attention_heads = 12
    config.se_config.intermediate_size = 3072
    config.se_config.hidden_dropout_prob = 0.1
    config.se_config.attention_probs_dropout_prob = 0.1
    config.se_config.is_decoder = True
    config.se_config.add_cross_attention = True
    config.se_config.chunk_size_feed_forward = 0
    config.se_config.use_self_cond = True
    
    # Data config
    config.data = ConfigDict()
    config.data.max_sequence_len = 80
    config.data.max_context_len = 80
    config.data.enc_gen_mean = None
    config.data.enc_gen_std = None
    
    # Dynamic config
    config.dynamic = ConfigDict()
    config.dynamic.N = 50
    config.dynamic.T = 1.0
    
    # Training config
    config.training = ConfigDict()
    config.training.checkpoints_folder = "checkpoints"
    config.training.checkpoints_prefix = "1tencdm-bert-base-cased-384-0.0002-rocstories-cfg=0.0"
    config.training.checkpoint_name = "last"
    
    # Validation config
    config.validation = ConfigDict()
    config.validation.batch_size = 4
    config.validation.cfg_coef = 0.0
    
    # Other configs
    config.emb = True
    config.use_self_cond = True
    config.ddp = False
    config.is_conditional = True
    config.timesteps = "linear"
    config.seed = 42
    
    return config


def test_model_loading(device='cpu'):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π –∏ –∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
    print("="*60)
    print("–ü–†–û–í–ï–†–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    
    config = create_test_config(device)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
    decoder_path = config.decoder.decoder_path
    checkpoint_path = os.path.join(
        config.training.checkpoints_folder,
        config.training.checkpoints_prefix,
        f"{config.training.checkpoint_name}.pth"
    )
    
    print(f"\n1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤:")
    print(f"   –î–µ–∫–æ–¥–µ—Ä: {decoder_path}")
    print(f"   –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(decoder_path)}")
    print(f"   –ß–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}")
    print(f"   –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(checkpoint_path)}")
    
    if not os.path.exists(decoder_path):
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –¥–µ–∫–æ–¥–µ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return False
    
    if not os.path.exists(checkpoint_path):
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return False
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–µ–∫–æ–¥–µ—Ä
    print(f"\n2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–µ–∫–æ–¥–µ—Ä–∞...")
    try:
        decoder_state = torch.load(decoder_path, map_location=device, weights_only=False)
        print(f"   ‚úì –î–µ–∫–æ–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        print(f"   –ö–ª—é—á–∏ –≤ state_dict: {list(decoder_state.keys())[:5]}...")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–µ–∫–æ–¥–µ—Ä–∞: {e}")
        return False
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
    print(f"\n3. –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞...")
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        print(f"   ‚úì –ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
        print(f"   –ö–ª—é—á–∏ –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ: {list(checkpoint.keys())}")
        if 'step' in checkpoint:
            print(f"   –®–∞–≥ –æ–±—É—á–µ–Ω–∏—è: {checkpoint['step']}")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
        return False
    
    print(f"\n‚úì –í—Å–µ —Ñ–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
    return True


def test_generation(device='cpu'):
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞"""
    print("\n" + "="*60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ì–ï–ù–ï–†–ê–¶–ò–ò")
    print("="*60)
    
    config = create_test_config(device)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print("\n1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(config.model.encoder_link)
    print(f"   ‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω: {tokenizer.__class__.__name__}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è —É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    test_sources = [
        "Once upon a time",
        "The weather was sunny and",
        "She decided to go to the",
    ]
    
    print(f"\n2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    for i, text in enumerate(test_sources):
        print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}: '{text}'")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    print(f"\n3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
    try:
        tok_src = tokenizer(
            test_sources,
            add_special_tokens=True,
            padding=True,
            truncation=True,
            max_length=config.data.max_context_len,
            return_tensors="pt",
            return_attention_mask=True,
            return_token_type_ids=False,
        )
        print(f"   ‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        print(f"   –§–æ—Ä–º–∞ input_ids: {tok_src['input_ids'].shape}")
        print(f"   –§–æ—Ä–º–∞ attention_mask: {tok_src['attention_mask'].shape}")
        print(f"   –ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–æ–≤: {tok_src['input_ids'][0][:10].tolist()}")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
        return False
    
    print(f"\n‚úì –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã!")
    return True


def full_generation_test(device='cpu'):
    """–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "="*60)
    print("–ü–û–õ–ù–´–ô –¢–ï–°–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò –° –ú–û–î–ï–õ–Ø–ú–ò")
    print("="*60)
    
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏
        from model.encoder import Encoder
        from model.decoder import BertDecoder
        from model.score_estimator import ScoreEstimatorEMB
        
        config = create_test_config(device)
        print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞
        print(f"\n1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞...")
        encoder = Encoder(
            config.model.encoder_link,
            enc_normalizer=None,
            is_change_sp_tokens=True,
            emb=config.emb
        ).eval()
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –±–µ–∑ –≤—ã–∑–æ–≤–∞ .cuda()
        if device != 'cpu':
            encoder = encoder.to(device)
        
        print(f"   ‚úì –≠–Ω–∫–æ–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {device}")
        
        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞
        print(f"\n2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞...")
        decoder = BertDecoder(
            decoder_config=config.decoder,
            diffusion_config=config.se_config
        )
        decoder_state = torch.load(config.decoder.decoder_path, map_location=device, weights_only=False)
        decoder.load_state_dict(decoder_state["decoder"])
        decoder = decoder.eval().to(device)
        print(f"   ‚úì –î–µ–∫–æ–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {device}")
        
        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è score estimator
        print(f"\n3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è score estimator...")
        score_estimator = ScoreEstimatorEMB(config=config.se_config).to(device)
        
        checkpoint_path = os.path.join(
            config.training.checkpoints_folder,
            config.training.checkpoints_prefix,
            f"{config.training.checkpoint_name}.pth"
        )
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º EMA –≤–µ—Å–∞
        if 'ema' in checkpoint:
            print(f"   –ó–∞–≥—Ä—É–∂–∞–µ–º EMA –≤–µ—Å–∞...")
            from utils.ema_model import ExponentialMovingAverage
            ema = ExponentialMovingAverage(score_estimator.parameters(), config.model.ema_rate)
            ema.load_state_dict(checkpoint["ema"])
            ema.copy_to(score_estimator.parameters())
        elif 'model' in checkpoint:
            print(f"   –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏...")
            score_estimator.load_state_dict(checkpoint["model"])
        
        score_estimator.eval()
        print(f"   ‚úì Score estimator –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {device} (—à–∞–≥: {checkpoint.get('step', 'N/A')})")
        
        # 4. –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        print(f"\n4. –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è...")
        tokenizer = AutoTokenizer.from_pretrained(config.model.encoder_link)
        
        test_text = "Once upon a time there was a"
        print(f"   –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç: '{test_text}'")
        
        with torch.no_grad():
            # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            tok = tokenizer(
                [test_text],
                add_special_tokens=True,
                padding=True,
                truncation=True,
                max_length=config.data.max_context_len,
                return_tensors="pt",
            ).to(device)
            
            src_x = encoder(
                input_ids=tok["input_ids"],
                attention_mask=tok["attention_mask"]
            )
            
            print(f"   ‚úì –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω")
            print(f"   –§–æ—Ä–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {src_x.shape}")
            print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {src_x.device}")
            print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:")
            print(f"      Mean: {src_x.mean().item():.6f}")
            print(f"      Std: {src_x.std().item():.6f}")
            print(f"      Min: {src_x.min().item():.6f}")
            print(f"      Max: {src_x.max().item():.6f}")
        
        print(f"\n‚úì –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ê–ë–û–¢–ê–ï–¢!")
        print(f"\n–°–≤–æ–¥–∫–∞:")
        print(f"  ‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        print(f"  ‚Ä¢ –≠–Ω–∫–æ–¥–µ—Ä: —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print(f"  ‚Ä¢ –î–µ–∫–æ–¥–µ—Ä: –∑–∞–≥—Ä—É–∂–µ–Ω")
        print(f"  ‚Ä¢ Score Estimator: –∑–∞–≥—Ä—É–∂–µ–Ω (EMA)")
        print(f"  ‚Ä¢ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ")
        
        return True
        
    except ImportError as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π: {e}")
        print(f"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –º–æ–¥—É–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã")
        return False
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("–ó–ê–ü–£–°–ö –¢–ï–°–¢–û–í\n")
    
    # –¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
    success1 = test_model_loading()
    
    # –¢–µ—Å—Ç 2: –ë–∞–∑–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    if success1:
        success2 = test_generation()
    else:
        print("\n‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏")
        success2 = False
    
    # –¢–µ—Å—Ç 3: –ü–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –º–æ–¥–µ–ª—è–º–∏
    if success1 and success2:
        success3 = full_generation_test()
    else:
        print("\n‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑-–∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –æ—à–∏–±–æ–∫")
        success3 = False
    
    # –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print("\n" + "="*60)
    print("–ò–¢–û–ì–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("="*60)
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π: {'‚úì OK' if success1 else '‚ùå FAILED'}")
    print(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {'‚úì OK' if success2 else '‚ùå FAILED'}")
    print(f"–ü–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {'‚úì OK' if success3 else '‚ùå FAILED'}")
    print("="*60)
    
    if success1 and success2 and success3:
        print("\nüéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
    else:
        print("\n‚ö†Ô∏è –ï–°–¢–¨ –ü–†–û–ë–õ–ï–ú–´ - —Å–º. –≤—ã–≤–æ–¥ –≤—ã—à–µ")
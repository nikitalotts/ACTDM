import torch
import os
import argparse
from ml_collections import ConfigDict
from transformers import AutoTokenizer, AutoConfig

def create_test_config(device='cpu'):
    config = ConfigDict()
    
    config.device = device
    
    config.model = ConfigDict()
    config.model.encoder_link = "bert-base-cased"
    config.model.ema_rate = 0.9999
    
    config.decoder = ConfigDict()
    config.decoder.mode = "transformer"
    config.decoder.num_hidden_layers = 3
    config.decoder.is_conditional = False
    config.decoder.decoder_path = "datasets/rocstories/decoder-bert-base-cased-80-transformer.pth"
    
    config.se_config = create_se_config()
    
    config.data = ConfigDict()
    config.data.max_sequence_len = 80
    config.data.max_context_len = 80
    config.data.enc_gen_mean = None
    config.data.enc_gen_std = None
    
    config.dynamic = ConfigDict()
    config.dynamic.N = 50
    config.dynamic.T = 1.0
    
    config.training = ConfigDict()
    config.training.checkpoints_folder = "checkpoints"
    config.training.checkpoints_prefix = "1tencdm-bert-base-cased-384-0.0002-rocstories-cfg=0.0"
    config.training.checkpoint_name = "last"
    
    config.validation = ConfigDict()
    config.validation.batch_size = 4
    config.validation.cfg_coef = 0.0
    
    config.emb = True
    config.use_self_cond = True
    config.ddp = False
    config.is_conditional = False
    config.timesteps = "linear"
    config.seed = 42
    
    return config

def create_se_config():
    se_config = AutoConfig.from_pretrained("bert-base-cased")
    se_config.attention_head_size = se_config.hidden_size / se_config.num_attention_heads
    se_config.is_conditional = False
    se_config.use_self_cond = True
    se_config.vocab_size = 28996
    return se_config

def debug_score_estimator(score_estimator, x_t, time_t, attention_mask):
    """–ü–æ—à–∞–≥–æ–≤–∞—è –æ—Ç–ª–∞–¥–∫–∞ score estimator"""
    print(f"\nüîç –î–ï–¢–ê–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê SCORE ESTIMATOR:")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    print(f"   –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(f"     x_t: {x_t.shape}, dtype: {x_t.dtype}, device: {x_t.device}")
    print(f"     time_t: {time_t.shape}, dtype: {time_t.dtype}, device: {time_t.device}")
    print(f"     attention_mask: {attention_mask.shape}, dtype: {attention_mask.dtype}, device: {attention_mask.device}")
    
    # –°–æ–∑–¥–∞–µ–º x_0_self_cond (self-conditioning) –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –µ–≥–æ —Ç—Ä–µ–±—É–µ—Ç
    batch_size, seq_len, hidden_size = x_t.shape
    x_0_self_cond = torch.randn(batch_size, seq_len, hidden_size).to(x_t.device)
    print(f"     x_0_self_cond: {x_0_self_cond.shape} (—Å–æ–∑–¥–∞–Ω –¥–ª—è self-conditioning)")
    
    # –ü—Ä–æ–±—É–µ–º –≤—ã–∑–≤–∞—Ç—å forward —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    try:
        print(f"\n   –ü–æ–ø—ã—Ç–∫–∞ –≤—ã–∑–æ–≤–∞ forward —Å self-conditioning...")
        with torch.no_grad():
            output = score_estimator.forward(
                x_t=x_t,
                time_t=time_t,
                attention_mask=attention_mask,
                x_0_self_cond=x_0_self_cond
            )
        print(f"   ‚úì Forward —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω!")
        print(f"   –í—ã—Ö–æ–¥: {output.shape}")
        return output
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ forward: {e}")
        
        # –ü—Ä–æ–±—É–µ–º –±–µ–∑ self-conditioning
        try:
            print(f"\n   –ü–æ–ø—ã—Ç–∫–∞ –≤—ã–∑–æ–≤–∞ forward –±–µ–∑ self-conditioning...")
            with torch.no_grad():
                output = score_estimator.forward(
                    x_t=x_t,
                    time_t=time_t,
                    attention_mask=attention_mask,
                    x_0_self_cond=None
                )
            print(f"   ‚úì Forward —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω –±–µ–∑ self-conditioning!")
            print(f"   –í—ã—Ö–æ–¥: {output.shape}")
            return output
        except Exception as e2:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏ –±–µ–∑ self-conditioning: {e2}")
            return None

def full_generation_test(device='cpu'):
    """–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "="*60)
    print("–ü–û–õ–ù–´–ô –¢–ï–°–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò –° –ú–û–î–ï–õ–Ø–ú–ò")
    print("="*60)
    
    try:
        from model.encoder import Encoder
        from model.decoder import BertDecoder
        from model.score_estimator import ScoreEstimatorEMB
        from utils.ema_model import ExponentialMovingAverage
        
        config = create_test_config(device)
        print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞
        print(f"\n1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞...")
        encoder = Encoder(
            config.model.encoder_link,
            enc_normalizer=None,
            is_change_sp_tokens=True,
            emb=config.emb
        ).eval()
        
        if device != 'cpu':
            encoder = encoder.to(device)
        
        print(f"   ‚úì –≠–Ω–∫–æ–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {device}")
        
        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞
        print(f"\n2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞...")
        
        decoder_state = torch.load(config.decoder.decoder_path, map_location=device, weights_only=False)
        
        decoder = BertDecoder(
            decoder_config=config.decoder,
            diffusion_config=config.se_config
        )
        
        if 'decoder' in decoder_state:
            missing_keys, unexpected_keys = decoder.load_state_dict(decoder_state['decoder'], strict=False)
        else:
            missing_keys, unexpected_keys = decoder.load_state_dict(decoder_state, strict=False)
        
        print(f"   ‚úì –î–µ–∫–æ–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {device}")
        print(f"   –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏: {len(missing_keys)}")
        print(f"   –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏: {len(unexpected_keys)}")
        
        decoder = decoder.eval().to(device)
        
        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è score estimator —Å EMA
        print(f"\n3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è score estimator —Å EMA...")
        
        score_estimator = ScoreEstimatorEMB(config=config.se_config).to(device)
        
        # –°–æ–∑–¥–∞–µ–º EMA –æ–±—ä–µ–∫—Ç
        ema = ExponentialMovingAverage(score_estimator.parameters(), config.model.ema_rate)
        
        checkpoint_path = os.path.join(
            config.training.checkpoints_folder,
            config.training.checkpoints_prefix,
            f"{config.training.checkpoint_name}.pth"
        )
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º EMA –≤–µ—Å–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
        if 'ema' in checkpoint:
            print(f"   –ó–∞–≥—Ä—É–∂–∞–µ–º EMA –≤–µ—Å–∞...")
            ema.load_state_dict(checkpoint["ema"])
            # –ö–æ–ø–∏—Ä—É–µ–º EMA –≤–µ—Å–∞ –≤ –º–æ–¥–µ–ª—å
            ema.copy_to(score_estimator.parameters())
            print(f"   ‚úì EMA –≤–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
            
        elif 'model' in checkpoint:
            print(f"   –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—ã—á–Ω—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏...")
            missing_model, unexpected_model = score_estimator.load_state_dict(checkpoint["model"], strict=False)
            print(f"   Model - –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏: {len(missing_model)}")
            print(f"   Model - –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏: {len(unexpected_model)}")
        
        score_estimator.eval()
        print(f"   ‚úì Score estimator –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {device} (—à–∞–≥: {checkpoint.get('step', 'N/A')})")
        
        # 4. –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        print(f"\n4. –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è...")
        tokenizer = AutoTokenizer.from_pretrained(config.model.encoder_link)
        
        test_text = "Once upon a time there was a"
        print(f"   –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç: '{test_text}'")
        
        with torch.no_grad():
            # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            tok = tokenizer(
                [test_text],
                add_special_tokens=True,
                padding=True,
                truncation=True,
                max_length=config.data.max_context_len,
                return_tensors="pt",
            ).to(device)
            
            src_x = encoder(
                input_ids=tok["input_ids"],
                attention_mask=tok["attention_mask"]
            )
            
            print(f"   ‚úì –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω")
            print(f"   –§–æ—Ä–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {src_x.shape}")
            print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {src_x.device}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –¥–µ–∫–æ–¥–µ—Ä
            print(f"\n5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–∫–æ–¥–µ—Ä–∞...")
            batch_size, seq_len = 2, 10
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∞ –Ω–µ —Ç–æ–∫–µ–Ω—ã), –∫–æ—Ç–æ—Ä—ã–µ –æ–∂–∏–¥–∞–µ—Ç –¥–µ–∫–æ–¥–µ—Ä
            # –§–æ—Ä–º–∞: [batch_size, seq_len, hidden_size]
            hidden_size = config.se_config.hidden_size
            test_input = torch.randn(batch_size, seq_len, hidden_size).to(device)

            if config.decoder.is_conditional:
                decoder_output = decoder(test_input, encoder_hidden_states=src_x, encoder_attention_mask=tok["attention_mask"])
            else:
                decoder_output = decoder(test_input)

            print(f"   ‚úì –î–µ–∫–æ–¥–µ—Ä –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω")
            print(f"   –í—Ö–æ–¥ –¥–µ–∫–æ–¥–µ—Ä–∞: {test_input.shape}, dtype: {test_input.dtype}")
            print(f"   –í—ã—Ö–æ–¥ –¥–µ–∫–æ–¥–µ—Ä–∞: {decoder_output.shape}, dtype: {decoder_output.dtype}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º score estimator —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
            print(f"\n6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ score estimator...")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è score estimator
            batch_size, seq_len = 2, 10
            hidden_size = config.se_config.hidden_size
            x_t = torch.randn(batch_size, seq_len, hidden_size).to(device)
            time_t = torch.rand(batch_size).to(device)
            attention_mask = torch.ones(batch_size, seq_len).to(device)
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
            score_output = debug_score_estimator(score_estimator, x_t, time_t, attention_mask)
            
            if score_output is not None:
                print(f"   ‚úì Score estimator —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
                print(f"   –í—ã—Ö–æ–¥: {score_output.shape}")
                print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã—Ö–æ–¥–∞: mean={score_output.mean().item():.4f}, std={score_output.std().item():.4f}")
            else:
                print(f"   ‚ö†Ô∏è Score estimator –∏–º–µ–µ—Ç –ø—Ä–æ–±–ª–µ–º—É, –Ω–æ —ç—Ç–æ –º–æ–∂–µ—Ç –Ω–µ –º–µ—à–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        
        print(f"\nüéâ –í–°–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´ –†–ê–ë–û–¢–ê–Æ–¢ –ö–û–†–†–ï–ö–¢–ù–û!")
        print(f"\n–°–≤–æ–¥–∫–∞:")
        print(f"  ‚Ä¢ –≠–Ω–∫–æ–¥–µ—Ä: ‚úì —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print(f"  ‚Ä¢ –î–µ–∫–æ–¥–µ—Ä: ‚úì —Ä–∞–±–æ—Ç–∞–µ—Ç") 
        print(f"  ‚Ä¢ Score Estimator: {'‚úì —Ä–∞–±–æ—Ç–∞–µ—Ç' if score_output is not None else '‚ö†Ô∏è —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏'}")
        print(f"  ‚Ä¢ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞: {device}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("–ó–ê–ü–£–°–ö –¢–ï–°–¢–û–í\n")
    
    success = full_generation_test()
    
    print("\n" + "="*60)
    print("–ò–¢–û–ì–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("="*60)
    print(f"–ü–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {'‚úì OK' if success else '‚ùå FAILED'}")
    print("="*60)
    
    if success:
        print("\nüéâ –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù –£–°–ü–ï–®–ù–û!")
        print("–í—Å–µ –º–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤!")
    else:
        print("\n‚ö†Ô∏è –ï–°–¢–¨ –ü–†–û–ë–õ–ï–ú–´ - —Å–º. –≤—ã–≤–æ–¥ –≤—ã—à–µ")